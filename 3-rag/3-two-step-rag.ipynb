{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus – Extract Vector Query from a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "AI_STUDIO_KEY = os.getenv(\"GOOGLE_AI_STUDIO_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "if(COHERE_API_KEY == None):\n",
    "    print(\"COHERE_API_KEY is missing. You can't complete this exercise without it.\")\n",
    "    print(\"Go to https://dashboard.cohere.com/api-keys to get your key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Query from Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere, os\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "def generate_query_from_promt(prompt):\n",
    "    response = co.chat(\n",
    "        message=prompt,\n",
    "        search_queries_only=True\n",
    "    )\n",
    "    \n",
    "    return response.search_queries[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to generate a query from a prompt\n",
    "# TODO: test generate_query_from_promt on \"Where do the tallest penguins live?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to generate a query from a prompt\n",
    "# TODO: test generate_query_from_promt on \"What can you tell me about zero shot training? Please answer based on the provided content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-step RAG with generated prompt\n",
    "### Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# Connect to the local instance deployed with Docker Compose\n",
    "client = weaviate.connect_to_local(\n",
    "    headers={\n",
    "       \"X-Google-Api-Key\": AI_STUDIO_KEY,\n",
    "    },\n",
    "    \n",
    "    # Set longer timeout\n",
    "    additional_config=weaviate.config.AdditionalConfig(timeout=(500, 1500))\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weaviate, os\n",
    "\n",
    "# client = weaviate.connect_to_wcs(\n",
    "#     cluster_url=os.getenv(\"WORKSHOP_DEMO_URL\"),\n",
    "#     auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WORKSHOP_DEMO_KEY_READ\")),\n",
    "\n",
    "#     headers={\n",
    "#        \"X-Palm-Api-Key\": AI_STUDIO_KEY,\n",
    "#     }\n",
    "\n",
    "#     # Set longer timeout\n",
    "#     additional_config=weaviate.config.AdditionalConfig(timeout=(500, 1500))\n",
    "# )\n",
    "\n",
    "# client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "def two_step_rag(user_prompt):\n",
    "    # Step 1\n",
    "    prompt = user_prompt + \" Please only use the provided content with this prompt. Don't make things up.\"\n",
    "    \n",
    "    generated_query = generate_query_from_promt(prompt)\n",
    "    print(\"=== Generated Query ===\")\n",
    "    print(f\"Generated query: {generated_query}\")\n",
    "\n",
    "    # Step 2\n",
    "    papers = client.collections.get(\"Papers\")\n",
    "\n",
    "    response = papers.generate.near_text(\n",
    "        query=generated_query,\n",
    "        limit=5,\n",
    "        grouped_task=prompt,\n",
    "        grouped_properties=[\"chunk\", \"title\"]\n",
    "    )\n",
    "\n",
    "    # TODO: implement rag near_text query\n",
    "    # 1. use generated_query\n",
    "    # 2a. set limit to 5 - to get top 5 results\n",
    "    # 2b. set auto_limit to 1 - to get the best group of results\n",
    "    # 3. use the prompt for the task\n",
    "    # 4. only provide text and title as grouped properties\n",
    "\n",
    "    # Print results\n",
    "    print(\"=== Generated Response ===\")\n",
    "    print(response.generated)\n",
    "\n",
    "    print(\"=== Source ===\")\n",
    "    for item in response.objects:\n",
    "        print(item.properties[\"title\"])\n",
    "        print(item.properties[\"chunk\"])\n",
    "        print(\"——————————————————————————————\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_rag(\"What can you tell me about zero shot training?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_rag(\"What are the challenges of zero shot training?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
